"""
Example test WITHOUT mocks - demonstrates why this approach is problematic.

‚ö†Ô∏è WARNING: This test hits the REAL database/file system
This is an ANTI-PATTERN for automated testing. Use this only as a learning example.
"""

import json
import os
import time
from django.test import TestCase, Client
from django.conf import settings
from django.db import connection
from django.test.utils import override_settings
from save_to_database.models import CSV


class UnmockedDatabaseUploadTest(TestCase):
    """
    ‚ùå BAD PRACTICE: This test uses real database and file system
    
    Problems with this approach:
    1. SLOW: Real DB I/O takes 100-1000x longer than mocked tests
    2. BRITTLE: Depends on external resources (DB connection, disk space, permissions)
    3. SIDE EFFECTS: Creates real files that persist, pollutes file system
    4. FLAKY: Can fail due to network issues, disk full, permission errors
    5. HARD TO CLEAN: Test data can leak between runs if cleanup fails
    6. ENVIRONMENT-DEPENDENT: Behavior differs between dev/CI/prod databases
    7. DANGEROUS: Could accidentally connect to production database
    8. COUPLING: Tests multiple layers (view ‚Üí model ‚Üí DB ‚Üí file system) at once
    """
    
    @classmethod
    def setUpClass(cls):
        """Setup once for the entire test class"""
        super().setUpClass()
        print("\n" + "="*80)
        print("üö® UNMOCKED TEST SUITE - SHOWING REAL PROBLEMS")
        print("="*80)
        print(f"üìç Database: {settings.DATABASES['default']['ENGINE']}")
        print(f"üìÅ Media Root: {settings.MEDIA_ROOT}")
        print(f"üîå Database Name: {settings.DATABASES['default']['NAME']}")
        print("="*80 + "\n")
    
    def setUp(self):
        """
        Setup runs BEFORE each test
        ‚ö†Ô∏è This creates real database connections
        """
        self.client = Client()
        self.url = '/save-to-database/create/'
        
        # Sample payload that will be converted to CSV and saved to disk
        self.test_payload = {
            'name': 'test_unmocked_upload',
            'source_json': {
                'DEMOGRAPHY': {
                    'subject_initials': 'TEST',
                    'sin': '999',
                    'gender': 'Male',
                    'age': '30'
                },
                'VITAL_SIGNS': {
                    'systolic_bp': '120',
                    'diastolic_bp': '80'
                }
            }
        }
        
        # Track performance metrics
        self.start_time = time.time()
        self.initial_query_count = len(connection.queries)
        
        # Count existing files before test
        csv_dir = os.path.join(settings.MEDIA_ROOT, 'datasets/csvs/')
        self.initial_file_count = len(os.listdir(csv_dir)) if os.path.exists(csv_dir) else 0
    
    def tearDown(self):
        """
        Cleanup runs AFTER each test
        ‚ö†Ô∏è If this fails, files/records persist forever (test pollution)
        """
        # Calculate performance impact
        end_time = time.time()
        duration_ms = (end_time - self.start_time) * 1000
        query_count = len(connection.queries) - self.initial_query_count
        
        # Count files after test
        csv_dir = os.path.join(settings.MEDIA_ROOT, 'datasets/csvs/')
        final_file_count = len(os.listdir(csv_dir)) if os.path.exists(csv_dir) else 0
        files_created = final_file_count - self.initial_file_count
        
        print(f"\n‚è±Ô∏è  Test Duration: {duration_ms:.2f}ms")
        print(f"üîç Database Queries: {query_count}")
        print(f"üìÑ Files Created: {files_created}")
        print(f"üíæ Total Files on Disk: {final_file_count}")
        
        # Attempt to clean up - but this can fail!
        cleanup_start = time.time()
        try:
            # Delete all CSV records created during test
            deleted_count = CSV.objects.filter(name__startswith='test_unmocked').delete()[0]
            cleanup_duration = (time.time() - cleanup_start) * 1000
            
            print(f"üßπ Cleanup: Deleted {deleted_count} records in {cleanup_duration:.2f}ms")
            
            # ‚ö†Ô∏è PROBLEM: Files may still exist on disk even after model deletion
            # Django FileField.delete() doesn't always work reliably
            
        except Exception as e:
            # ‚ö†Ô∏è PROBLEM: If cleanup fails, test data persists
            print(f"‚ùå Cleanup failed: {e}")
            print("‚ö†Ô∏è  Test pollution: Data will persist for next run!")
    
    def test_create_csv_with_real_database(self):
        """
        ‚ùå BAD: This test hits the real database and creates real files
        
        Why this is problematic:
        - Takes 500-2000ms instead of 5-20ms with mocks
        - Creates actual CSV file on disk at MEDIA_ROOT/datasets/csvs/
        - Writes to actual database (could be production if misconfigured!)
        - Can fail due to disk space, permissions, DB locks
        - Leaves behind test artifacts
        """
        
        print("\n" + "-"*80)
        print("TEST 1: Creating CSV with Real Database")
        print("-"*80)
        
        # ‚ö†Ô∏è SLOW: Real HTTP request ‚Üí Django view ‚Üí DB write ‚Üí File I/O
        request_start = time.time()
        response = self.client.post(
            self.url,
            data=json.dumps(self.test_payload),
            content_type='application/json'
        )
        request_duration = (time.time() - request_start) * 1000
        
        print(f"üì§ HTTP Request took: {request_duration:.2f}ms")
        
        # ‚ö†Ô∏è BRITTLE: This can fail for many reasons unrelated to code:
        # - Database connection timeout
        # - Disk full
        # - Permission denied
        # - File system locked
        # - Wrong DB credentials in .env
        
        self.assertEqual(response.status_code, 201)
        data = response.json()
        
        print(f"‚úì Response Status: {response.status_code}")
        print(f"‚úì Created Record ID: {data['id']}")
        
        # ‚ö†Ô∏è COUPLED: We're testing multiple layers at once:
        # 1. View layer (HTTP handling)
        # 2. Business logic (JSON ‚Üí CSV conversion)
        # 3. Model layer (CSV record creation)
        # 4. Database layer (INSERT query)
        # 5. File system layer (file write)
        
        self.assertIn('id', data)
        self.assertEqual(data['name'], 'test_unmocked_upload')
        
        # Verify record exists in REAL database
        db_query_start = time.time()
        csv_record = CSV.objects.get(id=data['id'])
        db_query_duration = (time.time() - db_query_start) * 1000
        
        print(f"üîç Database Query took: {db_query_duration:.2f}ms")
        
        self.assertEqual(csv_record.name, 'test_unmocked_upload')
        
        # ‚ö†Ô∏è DANGEROUS: This checks if a real file exists on disk
        self.assertTrue(csv_record.file)
        file_path = os.path.join(settings.MEDIA_ROOT, csv_record.file.name)
        
        file_check_start = time.time()
        file_exists = os.path.exists(file_path)
        file_check_duration = (time.time() - file_check_start) * 1000
        
        print(f"üìÅ File Check took: {file_check_duration:.2f}ms")
        print(f"üìç File Location: {file_path}")
        print(f"‚úì File Exists: {file_exists}")
        
        self.assertTrue(file_exists)
        
        # ‚ö†Ô∏è SIDE EFFECT: A real CSV file now exists on your machine
        # Location: MEDIA_ROOT/datasets/csvs/test_unmocked_upload.csv
        # This file will persist even after the test if cleanup fails
        
        # Read the actual file from disk to verify content
        file_read_start = time.time()
        with open(file_path, 'rb') as f:
            csv_content = f.read()
            file_size = len(csv_content)
        file_read_duration = (time.time() - file_read_start) * 1000
        
        print(f"üíæ File Read took: {file_read_duration:.2f}ms")
        print(f"üìä File Size: {file_size} bytes")
        
        self.assertIn(b'subject_initials', csv_content)
        self.assertIn(b'TEST', csv_content)
        
        print(f"‚ö†Ô∏è  TOTAL OPERATION: ~{request_duration + db_query_duration + file_check_duration + file_read_duration:.2f}ms")
        print("    Compare to mocked test: ~5-20ms (10-50x faster!)")
    
    def test_update_csv_with_real_database(self):
        """
        ‚ùå BAD: Creates real record, then updates it in real database
        
        Additional problems:
        - Depends on previous create operation succeeding
        - Creates TWO files (original + updated version)
        - Multiple database transactions
        """
        
        print("\n" + "-"*80)
        print("TEST 2: Updating CSV with Real Database")
        print("-"*80)
        
        # First create a record (real DB write + file creation)
        create_start = time.time()
        create_response = self.client.post(
            self.url,
            data=json.dumps(self.test_payload),
            content_type='application/json'
        )
        create_duration = (time.time() - create_start) * 1000
        
        csv_id = create_response.json()['id']
        print(f"üì§ CREATE took: {create_duration:.2f}ms")
        
        # Now update it (another real DB write + file replacement)
        updated_payload = {
            'name': 'test_unmocked_updated',
            'source_json': {
                'DEMOGRAPHY': {
                    'subject_initials': 'UPDATED',
                    'sin': '888'
                }
            }
        }
        
        update_start = time.time()
        update_response = self.client.put(
            f'/save-to-database/update/{csv_id}/',
            data=json.dumps(updated_payload),
            content_type='application/json'
        )
        update_duration = (time.time() - update_start) * 1000
        
        print(f"üì§ UPDATE took: {update_duration:.2f}ms")
        print(f"‚ö†Ô∏è  TOTAL: {create_duration + update_duration:.2f}ms for CREATE + UPDATE")
        print("    Mocked equivalent: ~10-30ms (20-40x faster!)")
        
        self.assertEqual(update_response.status_code, 200)
        
        # Check file system state
        csv_dir = os.path.join(settings.MEDIA_ROOT, 'datasets/csvs/')
        files = [f for f in os.listdir(csv_dir) if 'test_unmocked' in f]
        print(f"üìÅ Files on disk with 'test_unmocked': {len(files)}")
        print(f"   Files: {files}")
        print("‚ö†Ô∏è  Note: Old files may or may not be cleaned up automatically")
    
    def test_concurrent_database_access(self):
        """
        ‚ùå BAD: Shows how unmocked tests can have race conditions
        
        This test can fail randomly depending on:
        - Database transaction isolation
        - File system locks
        - Timing of operations
        """
        
        print("\n" + "-"*80)
        print("TEST 3: Concurrent Database Access")
        print("-"*80)
        
        # Create multiple records "simultaneously"
        responses = []
        total_start = time.time()
        
        for i in range(5):
            payload = {
                'name': f'test_unmocked_concurrent_{i}',
                'source_json': {'data': f'record_{i}'}
            }
            
            iteration_start = time.time()
            response = self.client.post(
                self.url,
                data=json.dumps(payload),
                content_type='application/json'
            )
            iteration_duration = (time.time() - iteration_start) * 1000
            
            print(f"  Record {i}: {iteration_duration:.2f}ms")
            responses.append(response)
        
        total_duration = (time.time() - total_start) * 1000
        avg_duration = total_duration / 5
        
        print(f"\n‚è±Ô∏è  Total Time: {total_duration:.2f}ms")
        print(f"üìä Average per record: {avg_duration:.2f}ms")
        print(f"‚ö†Ô∏è  With mocks: ~50-100ms total (10-20x faster)")
        
        # ‚ö†Ô∏è FLAKY: This can fail due to:
        # - Database lock timeouts
        # - File system contention
        # - Race conditions in file naming
        
        # Verify all were created in REAL database
        for i, response in enumerate(responses):
            self.assertEqual(response.status_code, 201, f"Record {i} failed")
        
        # Count real records in database
        query_start = time.time()
        count = CSV.objects.filter(name__startswith='test_unmocked_concurrent').count()
        query_duration = (time.time() - query_start) * 1000
        
        print(f"üîç Count Query took: {query_duration:.2f}ms")
        print(f"‚úì Records in DB: {count}")
        
        self.assertEqual(count, 5)
        
        # Check file system
        csv_dir = os.path.join(settings.MEDIA_ROOT, 'datasets/csvs/')
        files = [f for f in os.listdir(csv_dir) if 'test_unmocked_concurrent' in f]
        print(f"üìÅ CSV files created: {len(files)}")
        
        # Calculate disk usage
        total_size = sum(
            os.path.getsize(os.path.join(csv_dir, f)) 
            for f in files
        )
        print(f"üíæ Total disk space used: {total_size} bytes")
        print("‚ö†Ô∏è  These files will persist if cleanup fails!")
    
    def test_show_database_queries(self):
        """
        Shows exactly what database queries are executed (no mocks)
        Demonstrates the performance cost of real DB operations
        """
        
        print("\n" + "-"*80)
        print("TEST 4: Database Query Analysis")
        print("-"*80)
        
        # Enable query logging
        from django.conf import settings
        debug_setting = settings.DEBUG
        settings.DEBUG = True
        
        # Clear query log
        connection.queries_log.clear()
        
        # Make a request
        response = self.client.post(
            self.url,
            data=json.dumps(self.test_payload),
            content_type='application/json'
        )
        
        # Analyze queries
        queries = connection.queries
        print(f"\nüìä Total Queries Executed: {len(queries)}")
        print("\n" + "="*80)
        
        for i, query in enumerate(queries, 1):
            sql = query['sql']
            time_ms = float(query['time']) * 1000
            
            # Identify query type
            query_type = "‚ùì UNKNOWN"
            if 'INSERT' in sql:
                query_type = "‚ûï INSERT"
            elif 'SELECT' in sql:
                query_type = "üîç SELECT"
            elif 'UPDATE' in sql:
                query_type = "‚úèÔ∏è  UPDATE"
            elif 'DELETE' in sql:
                query_type = "‚ùå DELETE"
            
            print(f"\nQuery {i}: {query_type} ({time_ms:.2f}ms)")
            print(f"  SQL: {sql[:150]}...")
        
        print("\n" + "="*80)
        
        total_query_time = sum(float(q['time']) for q in queries) * 1000
        print(f"\n‚è±Ô∏è  Total DB Time: {total_query_time:.2f}ms")
        print(f"‚ö†Ô∏è  With mocks: 0ms (queries are mocked!)")
        
        # Restore debug setting
        settings.DEBUG = debug_setting
        
        self.assertEqual(response.status_code, 201)


class PerformanceComparisonDemo(TestCase):
    """
    Demonstrates the performance difference between mocked and unmocked tests
    """
    
    def test_unmocked_performance_summary(self):
        """
        Shows a summary of performance issues with unmocked tests
        """
        print("\n" + "="*80)
        print("üìä PERFORMANCE COMPARISON SUMMARY")
        print("="*80)
        
        print("\nüêå UNMOCKED TESTS (what you just saw):")
        print("  ‚Ä¢ Single CREATE: ~500-1000ms")
        print("  ‚Ä¢ Single UPDATE: ~500-1000ms")
        print("  ‚Ä¢ 5 Concurrent: ~2000-4000ms")
        print("  ‚Ä¢ 100 tests: ~50-100 SECONDS")
        print("  ‚Ä¢ Database queries: 5-20 per test")
        print("  ‚Ä¢ Files created: 1-5 per test")
        print("  ‚Ä¢ Disk I/O: Constant")
        print("  ‚Ä¢ Cleanup time: 100-500ms per test")
        
        print("\n‚ö° MOCKED TESTS (ideal):")
        print("  ‚Ä¢ Single CREATE: ~5-20ms")
        print("  ‚Ä¢ Single UPDATE: ~5-20ms")
        print("  ‚Ä¢ 5 Concurrent: ~50-100ms")
        print("  ‚Ä¢ 100 tests: ~1-3 SECONDS")
        print("  ‚Ä¢ Database queries: 0 (mocked)")
        print("  ‚Ä¢ Files created: 0 (mocked)")
        print("  ‚Ä¢ Disk I/O: None")
        print("  ‚Ä¢ Cleanup time: <1ms (no real resources)")
        
        print("\nüìà SPEED IMPROVEMENT:")
        print("  ‚Ä¢ 20-50x faster per test")
        print("  ‚Ä¢ 100 tests: 50 seconds ‚Üí 2 seconds (25x faster)")
        print("  ‚Ä¢ 1000 tests: 8+ minutes ‚Üí 20 seconds (24x faster)")
        
        print("\nüí° RELIABILITY IMPROVEMENT:")
        print("  ‚Ä¢ No DB connection failures")
        print("  ‚Ä¢ No disk space issues")
        print("  ‚Ä¢ No file permission errors")
        print("  ‚Ä¢ No cleanup failures")
        print("  ‚Ä¢ No test pollution")
        print("  ‚Ä¢ Can run in parallel")
        print("  ‚Ä¢ Works in CI without DB setup")
        
        print("\n" + "="*80)
        print("‚úÖ CONCLUSION: Always mock external dependencies!")
        print("="*80 + "\n")


# ============================================================================
# HOW TO RUN THIS TEST
# ============================================================================
#
# To see the performance problems in action:
#
#   python manage.py test save_to_database.test_example_unmocked -v 2
#
# The -v 2 flag shows detailed output including all print statements
#
# You'll see:
# - Exact timing for each operation
# - Number of database queries
# - Files created on disk
# - Real file paths
# - Cleanup issues
#
# Compare this to running mocked tests (if you have them):
#
#   python manage.py test save_to_database.test_view -v 2
#
# ============================================================================


class WhyMocksAreBetter(TestCase):
    """
    ‚úÖ BETTER APPROACH: Use mocks to avoid hitting real resources
    
    Benefits of mocking:
    1. FAST: Tests run in milliseconds, not seconds
    2. ISOLATED: No external dependencies (DB, file system, network)
    3. RELIABLE: No flaky failures due to environment issues
    4. SAFE: Can't accidentally affect production data
    5. FOCUSED: Tests one layer at a time
    6. REPEATABLE: Same results every time, anywhere
    7. PARALLEL: Can run tests concurrently without conflicts
    
    Example (not implemented here, but shows the pattern):
    
    from unittest.mock import patch, MagicMock
    
    @patch('save_to_database.views.CSV.objects.create')
    @patch('save_to_database.utility.json_to_csv_bytes')
    def test_create_csv_mocked(self, mock_json_to_csv, mock_create):
        # Mock the CSV conversion
        mock_json_to_csv.return_value = b'csv,data'
        
        # Mock the database save
        mock_csv = MagicMock()
        mock_csv.id = 1
        mock_csv.name = 'test'
        mock_csv.file.url = '/media/test.csv'
        mock_create.return_value = mock_csv
        
        # Now test runs in <5ms with no real DB/file I/O
        response = self.client.post(...)
        
        # Verify mocks were called correctly
        mock_json_to_csv.assert_called_once()
        mock_create.assert_called_once()
    """
    
    def test_mock_example_placeholder(self):
        """
        This is just a placeholder to show the concept.
        Real mocked tests would be in separate test files.
        """
        self.assertTrue(True)


# ============================================================================
# SUMMARY: Why Unmocked Tests Are Problematic
# ============================================================================
#
# 1. PERFORMANCE
#    - Unmocked: 500-2000ms per test
#    - Mocked: 5-20ms per test
#    - 100+ tests? Unmocked takes 5+ minutes, mocked takes 5 seconds
#
# 2. RELIABILITY
#    - Unmocked: Fails due to DB issues, disk space, permissions, network
#    - Mocked: Only fails if code is actually broken
#
# 3. ISOLATION
#    - Unmocked: Tests affect each other via shared DB/file state
#    - Mocked: Each test is completely independent
#
# 4. SAFETY
#    - Unmocked: Can accidentally write to production DB
#    - Mocked: No external systems touched
#
# 5. FOCUS
#    - Unmocked: Tests everything at once (hard to debug)
#    - Mocked: Tests one component at a time (easy to debug)
#
# 6. CI/CD
#    - Unmocked: Requires DB setup in CI, can't run in parallel
#    - Mocked: Runs anywhere, any time, in parallel
#
# ============================================================================
